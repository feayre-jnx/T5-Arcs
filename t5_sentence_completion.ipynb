{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Completion Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-small\", device_map={\"\":0})\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\", device_map={\"\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class that tokenizes the data\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_addr, tokenizer, max_input_length=128, max_target_length=128, mask_prob=0.3):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_target_length = max_target_length\n",
    "        self.mask_prob = mask_prob\n",
    "\n",
    "        df_frame = pd.read_csv(data_addr)\n",
    "        input_list = []\n",
    "        for i in range(df_frame.shape[0]):\n",
    "            input_temp = ('Type:' + df_frame.iloc[i,0] + ' Series:' + str(df_frame.iloc[i,1]) + ' Color:' +  df_frame.iloc[i,2] + ' Buff:' +  \n",
    "                    str(df_frame.iloc[i,3]) + ' Material:' + df_frame.iloc[i,4] + ' Force:' +  str(df_frame.iloc[i,5]) + ' Range:' + str(df_frame.iloc[i,6])) + ' Attachment:' + str(df_frame.iloc[i,7])\n",
    "\n",
    "            input_list.append(input_temp)\n",
    "\n",
    "        self.dataset = {\n",
    "            'input': input_list,\n",
    "            }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset['input'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_text = self.dataset['input'][idx]\n",
    "        target_text = self.dataset['input'][idx]\n",
    "\n",
    "        if random.random() < self.mask_prob:\n",
    "            input_text = self.mask_input(input_text)\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            input_text, \n",
    "            max_length=self.max_input_length, \n",
    "            truncation=True, \n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Tokenize inputs and targets\n",
    "        targets = self.tokenizer(\n",
    "            target_text, \n",
    "            max_length=self.max_target_length, \n",
    "            truncation=True, \n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": targets[\"input_ids\"].squeeze()\n",
    "        }\n",
    "    \n",
    "    def mask_input(self, input_text, mask_no=4):\n",
    "\n",
    "        parsed_input = input_text.split(' ')\n",
    "        parsed_input = [i.split(':') for i in parsed_input]\n",
    "\n",
    "        ## randomly choose from the list\n",
    "        idx_choices = list(range(len(parsed_input)))\n",
    "\n",
    "        ## mask the value\n",
    "        for i in range(mask_no):\n",
    "            idx = idx_choices.pop(random.choice(range(len(idx_choices))))\n",
    "            parsed_input[idx][1] = '<extra_id_' + str(i) + '>'\n",
    "\n",
    "        ## return to the input format\n",
    "        new_input = ''\n",
    "        for i in range(len(parsed_input)):\n",
    "            new_input = new_input + parsed_input[i][0] + ':' + parsed_input[i][1]\n",
    "            if i < len(parsed_input)-1:\n",
    "                new_input = new_input + ' '\n",
    "\n",
    "        return new_input\n",
    "\n",
    "# Define the custom data collator that applies dynamic augmentation\n",
    "class DataCollator(DataCollatorForSeq2Seq):\n",
    "    def __init__(self, tokenizer, model):\n",
    "        super().__init__(tokenizer, model)\n",
    "\n",
    "    \n",
    "    def __call__(self, features):\n",
    "        # Convert list of dicts to dict of tensors\n",
    "        batch = super().__call__(features)\n",
    "        \n",
    "        return batch\n",
    "\n",
    "# Prepare the custom dataset\n",
    "dataset = CustomDataset('data-item-only.csv', tokenizer, 128, 128, mask_prob=.5)\n",
    "\n",
    "# Prepare the augmenting data collator\n",
    "data_collator = DataCollator(tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training arguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "            output_dir='./results/exp',\n",
    "            num_train_epochs=50,\n",
    "            per_device_train_batch_size=8,\n",
    "            per_device_eval_batch_size=8,\n",
    "            warmup_steps=500,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir='./logs',\n",
    "            learning_rate= 5e-6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34589/1495217669.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/data/data_collator.py:741: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1700' max='1700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1700/1700 01:33, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.028800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.027100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.027500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1700, training_loss=0.027844177694881664, metrics={'train_runtime': 93.469, 'train_samples_per_second': 141.758, 'train_steps_per_second': 18.188, 'total_flos': 448319717376000.0, 'train_loss': 0.027844177694881664, 'epoch': 50.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## init trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer\n",
    "\n",
    ")\n",
    "\n",
    "trainer.args._n_gpu = 1\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./weights/exp/t5_sc_tokenizer/tokenizer_config.json',\n",
       " './weights/exp/t5_sc_tokenizer/special_tokens_map.json',\n",
       " './weights/exp/t5_sc_tokenizer/spiece.model',\n",
       " './weights/exp/t5_sc_tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## save the model weights\n",
    "\n",
    "model.save_pretrained('./weights/exp/t5_sc_model')\n",
    "tokenizer.save_pretrained('./weights/exp/t5_sc_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OPTIONAL: Load weights \n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('./weights/exp/t5_sc_tokenizer', device_map={\"\":0})\n",
    "model = T5ForConditionalGeneration.from_pretrained('./weights/exp/t5_sc_model', device_map={\"\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## inference function\n",
    "\n",
    "def generate_completion(input_text, temperature=1):\n",
    "    input_ids= tokenizer(input_text, return_tensors='pt').input_ids\n",
    "\n",
    "    input_ids = input_ids.to('cuda:0')\n",
    "    outputs = model.generate(input_ids, max_length=128, temperature=temperature, do_sample=True)#, num_beams=5)\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temp:  0.5\n",
      "Input:  Type:<extra_id_0> Series:Mach Color:<extra_id_4> Buff:x2 Material:<extra_id_6> Force:10 Range:40 Attachment:1\n",
      "Generated Completion:  Type:Dagger Series:Mach Color:White Buff:x2 Material:Iron Force:10 Range:40 Attachment:1\n",
      "\n",
      "\n",
      "Temp:  1\n",
      "Input:  Type:<extra_id_0> Series:Mach Color:<extra_id_4> Buff:x2 Material:<extra_id_6> Force:10 Range:40 Attachment:1\n",
      "Generated Completion:  Type:Longsword Series:Mach Color:White Buff:x2 Material:Steel Force:10 Range:40 Attachment:1\n",
      "\n",
      "\n",
      "Temp:  1.25\n",
      "Input:  Type:<extra_id_0> Series:Mach Color:<extra_id_4> Buff:x2 Material:<extra_id_6> Force:10 Range:40 Attachment:1\n",
      "Generated Completion:  Type:Spear Series:Mach Color:Black Buff:x2 Material:Mach Color:White Buff:x2 Material:Illust Force:10 Range:40 Attachment:1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Test model for missing information completion\n",
    "\n",
    "input_text = 'Type:<extra_id_0> Series:Mach Color:<extra_id_4> Buff:x2 Material:<extra_id_6> Force:10 Range:40 Attachment:1'\n",
    "\n",
    "output = []\n",
    "for i, temp in enumerate([0.5, 1, 1.25]):\n",
    "    output.append(generate_completion(input_text, temperature=temp)) \n",
    "    print('Temp: ', temp)\n",
    "    print('Input: ', input_text)\n",
    "    print('Generated Completion: ', output[i])\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
