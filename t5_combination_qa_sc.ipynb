{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combination: Question Answering and Sentence Completion Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-small\", device_map={\"\":0})\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\", device_map={\"\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class that tokenizes the data\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_addr_item, data_addr_comp, tokenizer, max_input_length=128, max_target_length=128, mask_prob=0.3):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_target_length = max_target_length\n",
    "        self.mask_prob = mask_prob\n",
    "\n",
    "        df_item = pd.read_csv(data_addr_item)\n",
    "        input_list_item = []\n",
    "        for i in range(df_item.shape[0]):\n",
    "            input_temp = ('Type:' + df_item.iloc[i,0] + ' Series:' + str(df_item.iloc[i,1]) + ' Color:' +  df_item.iloc[i,2] + ' Buff:' +  \n",
    "                    str(df_item.iloc[i,3]) + ' Material:' + df_item.iloc[i,4] + ' Force:' +  str(df_item.iloc[i,5]) + ' Range:' + str(df_item.iloc[i,6])) + ' Attachment:' + str(df_item.iloc[i,7])\n",
    "\n",
    "            input_list_item.append(input_temp)\n",
    "\n",
    "\n",
    "        df_comp = pd.read_csv(data_addr_comp)\n",
    "        input_list_comp, target_list_comp = [], []\n",
    "        for i in range(df_comp.shape[0]):\n",
    "            input_temp = ('Type:' + df_comp.iloc[i,0] + ' Series:' + str(df_comp.iloc[i,1]) + ' Color:' +  df_comp.iloc[i,2] + ' Buff:' +  \n",
    "                    str(df_comp.iloc[i,3]) + ' Material:' + df_comp.iloc[i,4] + ' Force:' +  str(df_comp.iloc[i,5]) + ' Range:' + str(df_comp.iloc[i,6])) + ' Attachment:' + str(df_comp.iloc[i,7])\n",
    "            target_temp = ('Type:' + df_comp.iloc[i,8] + ' Series_Comp:' + str(df_comp.iloc[i,9]) + ' Force_Comp:' + \n",
    "                        str(df_comp.iloc[i,10]) + ' Range_Comp:' + str(df_comp.iloc[i,11]) + ' Source:' + df_comp.iloc[i,12])\n",
    "\n",
    "            input_list_comp.append(input_temp)\n",
    "            target_list_comp.append(target_temp)\n",
    "\n",
    "        self.dataset = {\n",
    "            'input_item': input_list_item,\n",
    "            'input_comp': input_list_comp,\n",
    "            'target_comp': target_list_comp,\n",
    "            }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset['input_item']) + len(self.dataset['input_comp'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ## determine if the dataset will come from task1 or task2 through the idx\n",
    "        ## if the idx is more than the len of self.dataset['input_item'] then it will go to task2\n",
    "        \n",
    "        if idx < len(self.dataset['input_item']):\n",
    "            ## Task 1\n",
    "\n",
    "            input_text = self.dataset['input_item'][idx]\n",
    "            target_text = self.dataset['input_item'][idx]\n",
    "\n",
    "            if random.random() < self.mask_prob:\n",
    "                input_text = self.mask_input(input_text)\n",
    "\n",
    "            ## add the task prefix\n",
    "\n",
    "            ## complete item\n",
    "            input_text = 'complete_item: ' + input_text\n",
    "\n",
    "            inputs = self.tokenizer(\n",
    "                input_text, \n",
    "                max_length=self.max_input_length, \n",
    "                truncation=True, \n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            # Tokenize inputs and targets\n",
    "            targets = self.tokenizer(\n",
    "                target_text, \n",
    "                max_length=self.max_target_length, \n",
    "                truncation=True, \n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            ## Task 2\n",
    "            \n",
    "            ## adjust the index to go back to zero\n",
    "            new_idx = idx - len(self.dataset['input_item'])\n",
    "\n",
    "            input_text = self.dataset['input_comp'][new_idx]\n",
    "            target_text = self.dataset['target_comp'][new_idx]\n",
    "\n",
    "            ## generate corresponding element \n",
    "            input_text = 'generate_element: ' + input_text\n",
    "\n",
    "            inputs = self.tokenizer(\n",
    "                input_text, \n",
    "                max_length=self.max_input_length, \n",
    "                truncation=True, \n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            # Tokenize inputs and targets\n",
    "            targets = self.tokenizer(\n",
    "                target_text, \n",
    "                max_length=self.max_target_length, \n",
    "                truncation=True, \n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": targets[\"input_ids\"].squeeze()\n",
    "        }\n",
    "    \n",
    "    def mask_input(self, input_text, mask_no=4):\n",
    "\n",
    "        parsed_input = input_text.split(' ')\n",
    "        parsed_input = [i.split(':') for i in parsed_input]\n",
    "\n",
    "        ## randomly choose from the list\n",
    "        idx_choices = list(range(len(parsed_input)))\n",
    "\n",
    "        ## mask the value\n",
    "        for i in range(mask_no):\n",
    "            idx = idx_choices.pop(random.choice(range(len(idx_choices))))\n",
    "            parsed_input[idx][1] = '<extra_id_' + str(i) + '>'\n",
    "\n",
    "        ## return to the input format\n",
    "        new_input = ''\n",
    "        for i in range(len(parsed_input)):\n",
    "            new_input = new_input + parsed_input[i][0] + ':' + parsed_input[i][1]\n",
    "            if i < len(parsed_input)-1:\n",
    "                new_input = new_input + ' '\n",
    "\n",
    "        return new_input\n",
    "\n",
    "# Define the custom data collator that applies dynamic augmentation\n",
    "class DataCollator(DataCollatorForSeq2Seq):\n",
    "    def __init__(self, tokenizer, model):\n",
    "        super().__init__(tokenizer, model)\n",
    "\n",
    "    \n",
    "    def __call__(self, features):\n",
    "        # Convert list of dicts to dict of tensors\n",
    "        batch = super().__call__(features)\n",
    "        \n",
    "        return batch\n",
    "\n",
    "# Prepare the custom dataset\n",
    "dataset = CustomDataset('data-item-only.csv', 'data.csv', tokenizer, 128, 128, mask_prob=.5)\n",
    "\n",
    "# Prepare the augmenting data collator\n",
    "data_collator = DataCollator(tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training arguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "            output_dir='./results/exp',\n",
    "            num_train_epochs=10,\n",
    "            per_device_train_batch_size=8,\n",
    "            per_device_eval_batch_size=8,\n",
    "            warmup_steps=500,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir='./logs',\n",
    "            learning_rate= 5e-7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12567/1495217669.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/data/data_collator.py:741: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2140' max='2140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2140/2140 01:31, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.028400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.030800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2140, training_loss=0.029755637356054004, metrics={'train_runtime': 92.1601, 'train_samples_per_second': 185.764, 'train_steps_per_second': 23.22, 'total_flos': 579262910300160.0, 'train_loss': 0.029755637356054004, 'epoch': 10.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## init trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer\n",
    "\n",
    ")\n",
    "\n",
    "trainer.args._n_gpu = 1\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./weights/exp/t5_qa_sc_tokenizer/tokenizer_config.json',\n",
       " './weights/exp/t5_qa_sc_tokenizer/special_tokens_map.json',\n",
       " './weights/exp/t5_qa_sc_tokenizer/spiece.model',\n",
       " './weights/exp/t5_qa_sc_tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## save the model weights\n",
    "\n",
    "model.save_pretrained('./weights/exp/t5_qa_sc_model')\n",
    "tokenizer.save_pretrained('./weights/exp/t5_qa_sc_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OPTIONAL: Load weights \n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('./weights/exp/t5_qa_sc_tokenizer', device_map={\"\":0})\n",
    "model = T5ForConditionalGeneration.from_pretrained('./weights/exp/t5_qa_sc_model', device_map={\"\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## inference function\n",
    "\n",
    "def generate_pairs(input_text, task_prefix, temperature=1):\n",
    "    input_ids= tokenizer(task_prefix + input_text, return_tensors='pt').input_ids\n",
    "\n",
    "    input_ids = input_ids.to('cuda:0')\n",
    "    outputs = model.generate(input_ids, max_length=128, temperature=temperature, do_sample=True)#, num_beams=5)\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temp:  0.5\n",
      "Input:  Type:<extra_id_0> Series:Mach Color:<extra_id_4> Buff:x2 Material:<extra_id_6> Force:10 Range:40 Attachment:1\n",
      "Generated Completion:  Type:Longsword Series:Mach Color:White Buff:x2 Material:Steel Force:10 Range:40 Attachment:1\n",
      "\n",
      "\n",
      "Temp:  0.6\n",
      "Input:  Type:<extra_id_0> Series:Mach Color:<extra_id_4> Buff:x2 Material:<extra_id_6> Force:10 Range:40 Attachment:1\n",
      "Generated Completion:  Type:Dagger Series:Mach Color:White Buff:x2 Material:Steel Force:10 Range:40 Attachment:1\n",
      "\n",
      "\n",
      "Temp:  0.7\n",
      "Input:  Type:<extra_id_0> Series:Mach Color:<extra_id_4> Buff:x2 Material:<extra_id_6> Force:10 Range:40 Attachment:1\n",
      "Generated Completion:  Type:Dagger Series:Mach Color:White Buff:x2 Material:Steel Force:10 Range:40 Attachment:1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Task 1\n",
    "## Complete the item information by replacing the placeholder <extra_id_#> with an existing value\n",
    "\n",
    "input_text = 'Type:<extra_id_0> Series:Mach Color:<extra_id_1> Buff:x2 Material:<extra_id_2> Force:10 Range:40 Attachment:1'\n",
    "\n",
    "task_prefix = \"complete_item: \"\n",
    "output_item = []\n",
    "for i, temp in enumerate([0.5, 0.6, .7]):\n",
    "    output_item.append(generate_pairs(input_text, task_prefix, temperature=temp)) \n",
    "    print('Temp: ', temp)\n",
    "    print('Input: ', input_text)\n",
    "    print('Generated Completion: ', output_item[i])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate Item: Type:Longsword Series:Mach Color:White Buff:x2 Material:Steel Force:10 Range:40 Attachment:1\n",
      "Temp:  0.5\n",
      "Item Input:  Type:Longsword Series:Mach Color:White Buff:x2 Material:Steel Force:10 Range:40 Attachment:1\n",
      "Generated Complement:  Type:Fire Series_Comp:Mach Force_Comp:20 Range_Comp:40 Source:Steampunk\n",
      "\n",
      "\n",
      "Temp:  0.75\n",
      "Item Input:  Type:Longsword Series:Mach Color:White Buff:x2 Material:Steel Force:10 Range:40 Attachment:1\n",
      "Generated Complement:  Type:Electric Series_Comp:Mach Force_Comp:20 Range_Comp:40 Source:Traditional\n",
      "\n",
      "\n",
      "Temp:  1\n",
      "Item Input:  Type:Longsword Series:Mach Color:White Buff:x2 Material:Steel Force:10 Range:40 Attachment:1\n",
      "Generated Complement:  Type:Electric Series_Comp:Mach Force_Comp:40 Range_Comp:40 Source:Traditional\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Candidate Item: Type:Dagger Series:Mach Color:White Buff:x2 Material:Steel Force:10 Range:40 Attachment:1\n",
      "Temp:  0.5\n",
      "Item Input:  Type:Dagger Series:Mach Color:White Buff:x2 Material:Steel Force:10 Range:40 Attachment:1\n",
      "Generated Complement:  Type:Fire Series_Comp:Mach Force_Comp:20 Range_Comp:40 Source:Steampunk\n",
      "\n",
      "\n",
      "Temp:  0.75\n",
      "Item Input:  Type:Dagger Series:Mach Color:White Buff:x2 Material:Steel Force:10 Range:40 Attachment:1\n",
      "Generated Complement:  Type:Electric Series_Comp:Mach Force_Comp:20 Range_Comp:40 Source:Traditional\n",
      "\n",
      "\n",
      "Temp:  1\n",
      "Item Input:  Type:Dagger Series:Mach Color:White Buff:x2 Material:Steel Force:10 Range:40 Attachment:1\n",
      "Generated Complement:  Type:Electric Series_Comp:Mach Force_Comp:40 Range_Comp:40 Source:Traditional\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Candidate Item: Type:Dagger Series:Mach Color:White Buff:x2 Material:Steel Force:10 Range:40 Attachment:1\n",
      "Temp:  0.5\n",
      "Item Input:  Type:Dagger Series:Mach Color:White Buff:x2 Material:Steel Force:10 Range:40 Attachment:1\n",
      "Generated Complement:  Type:Fire Series_Comp:Mach Force_Comp:20 Range_Comp:40 Source:Steampunk\n",
      "\n",
      "\n",
      "Temp:  0.75\n",
      "Item Input:  Type:Dagger Series:Mach Color:White Buff:x2 Material:Steel Force:10 Range:40 Attachment:1\n",
      "Generated Complement:  Type:Electric Series_Comp:Mach Force_Comp:20 Range_Comp:40 Source:Traditional\n",
      "\n",
      "\n",
      "Temp:  1\n",
      "Item Input:  Type:Dagger Series:Mach Color:White Buff:x2 Material:Steel Force:10 Range:40 Attachment:1\n",
      "Generated Complement:  Type:Electric Series_Comp:Mach Force_Comp:40 Range_Comp:40 Source:Traditional\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Task 2 \n",
    "## Use the completed item to search for its corresponding element\n",
    "\n",
    "task_prefix = \"generate_element: \"\n",
    "output_comp = []\n",
    "\n",
    "for j in output_item:\n",
    "    input_text = j\n",
    "    print('Candidate Item: ' + j)\n",
    "    for i, temp in enumerate([0.5, .75, 1]):\n",
    "        output_comp.append(generate_pairs(input_text, task_prefix, temperature=temp)) \n",
    "        print('Temp: ', temp)\n",
    "        print('Item Input: ', input_text)\n",
    "        print('Generated Complement: ', output_comp[i])\n",
    "        print('\\n')\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
